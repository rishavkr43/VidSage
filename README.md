# ğŸ¬ VidSage

VidSage is a Chrome extension + backend service that lets you **chat with YouTube videos**.  
It extracts transcripts, indexes them with FAISS, and uses **Google Gemini** to answer questions with context.

-*-*-*-*-*

## ğŸš€ Features
- Fetch YouTube transcripts automatically
- Split and embed transcripts using FAISS
- Ask contextual questions while watching a video
- Interactive Chrome extension UI (3D styled with Three.js)
- Maintains conversation history across questions

---

## ğŸ“‚ Project Structure
```bash
VidSage/
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py                    # FastAPI app
â”‚   â”‚   â””â”€â”€ transcript.py              # Transcript fetching
â”‚   â”‚
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ schemas.py                 # Pydantic models
â”‚   â”‚â”€â”€ .env
â”‚   â””â”€â”€ rag.py                         # Embeddings + retrieval + Gemini
â”‚
â”œâ”€â”€ extension/
â”‚   â”œâ”€â”€ assets/
â”‚   â”‚   â”œâ”€â”€ icons/                     # Avatar and icon images
â”‚   â”‚   â”‚   â”œâ”€â”€ icon1.png
â”‚   â”‚   â”‚   â”œâ”€â”€ icon2.png
â”‚   â”‚   â”‚   â””â”€â”€ icon16.png
â”‚   â”‚   â””â”€â”€ backgrounds/
â”‚   â”‚
â”‚   â”œâ”€â”€ libs/                          # Third-party libs (e.g. three.min.js)
â”‚   â”‚   â””â”€â”€ three.min.js
â”‚   â”‚
â”‚   â”œâ”€â”€ three-scenes/                  # Optional Three.js scene for visuals
â”‚   â”‚
â”‚   â”œâ”€â”€ content-scripts.js             # Injects floating button into YouTube pages
â”‚   â”‚
â”‚   â”œâ”€â”€ manifest.json                  # Extension manifest and permissions
â”‚   â”‚
â”‚   â”œâ”€â”€ ui.css                         # Styles for the extension UI
â”‚   â”‚
â”‚   â”œâ”€â”€ ui.html                        # UI markup (popup or full-tab page)
â”‚   â”‚
â”‚   â””â”€â”€ ui.js                          # Frontend logic and messaging
â”‚
â”œâ”€â”€ requirements.txt
|
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```


-*-*-*-*-*

## ğŸ› ï¸ Setup

### 1. Backend
# Install deps (To run Locally )
pip install -r requirements.txt

# Run the server
uvicorn app.main:app --reload --port 8000
Server runs at: http://127.0.0.1:8000
Docs available at: http://127.0.0.1:8000/docs

2. Environment

Add your Gemini API key in .env inside backend Folder:

GEMINI_API_KEY="your-gemini-api-key-here"

3. Chrome Extension

Go to chrome://extensions/

Enable Developer mode

Click Load unpacked

Select the extension/ folder

Pin VidSage to toolbar

ğŸ“Œ Example Usage

Open a YouTube video

Click VidSage icon

Ask: â€œSummarize this video in 3 pointsâ€

Get contextual Gemini-powered answers ğŸ‰

-*-*-*-*-*
## ğŸ¤– How The AI Answers (and Why Itâ€™s Restricted)
VidSage is designed to answer questions *only* using information present in the video's transcript. The AI does not freely invent facts â€” this restriction prevents hallucinations and keeps answers grounded in the source material.

- **What the AI does:** When you ask a question, the system retrieves relevant transcript snippets, builds a context-aware prompt, and asks Gemini to generate an answer using that exact context.

- **What the AI does not do:** It will not fabricate answers from outside knowledge. If the transcript does not contain the requested information, the AI will respond that it cannot find an answer in the video rather than guessing.

Why this restriction matters
- Large language models (LLMs) are excellent at producing plausible-sounding text. Without strict grounding, they can confidently invent details ("hallucinate") that are not supported by the source.
- By constraining Gemini to only use retrieved transcript text, VidSage avoids presenting users with misleading or false claims.

What happens if you donâ€™t restrict it

If you allow the AI to answer freely without grounding, it can and will fill gaps with plausible-sounding but potentially incorrect information. For example:

- Video topic: "AI in healthcare" (transcript discusses models, workflow, and ethics)
- User question: "What year did the iPhone launch?"

	- Without restrictions â†’ the AI may still answer "2007" (a correct fact, but unrelated to the video), or worse, invent a wrong year with high confidence.
	- With restrictions â†’ the AI should respond: "I don't know â€” the video transcript doesn't mention that." or "I can't find this information in the video."

This behavior protects users from being misled and preserves trust in the tool.

Best practices
- Phrase questions that are likely to be covered in the video (topics, names, events mentioned). If you need general knowledge beyond the video, ask explicitly that outside sources may be used.
- If a user needs external facts, build a separate QA flow that allows the model to consult broader sources (and clearly present that the answer used external knowledge).

-*-*-*-*-*
